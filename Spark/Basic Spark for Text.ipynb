{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BD3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqesZPZKslCQ"
      },
      "source": [
        "## Getting Spark ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ExQ6kat0eeTt"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jo2QEuV6fjra"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "utbntKibiFBt",
        "outputId": "40dd5210-4cb1-499a-c310-6c1611cb813e"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://20e1e3a14861:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd748056710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4MPhw3aqfj-O",
        "outputId": "fc1bc405-9182-427c-a6db-bffc0b1777f9"
      },
      "source": [
        "spark.sparkContext.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.1.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1w3RVxBszd"
      },
      "source": [
        "## Part 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYp3Uf__aLij"
      },
      "source": [
        "\n",
        "\n",
        "First, textfile is read and spread across 4 workers.\n",
        "\n",
        "Then, it is preprocessed. Each element in 'book' array contains a line or a sentence.\n",
        "\n",
        "floatMap splits the lines to words and now 'words' is an array of all the words in the book.\n",
        "\n",
        "number of all words in the text is returned. it is 5020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZXmAVjbt7Xb",
        "outputId": "6d37e7c3-49a8-4686-a041-7e395bb6d518"
      },
      "source": [
        "import re\n",
        "\n",
        "# Preprocessing text (remove punctuation, lowercase words, and strip extra spaces)\n",
        "def removePunctuation(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^a-z\\s\\d]', \"\", text)\n",
        "  return text.strip()\n",
        "\n",
        "book = spark.sparkContext.textFile('input.txt', 4).map(removePunctuation)\n",
        "words = book.flatMap(lambda line: line.split(\" \")) \n",
        "\n",
        "# Number of words\n",
        "all_count = words.count()\n",
        "print(all_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxseL30xsi1L"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiHoRDT3T0Td"
      },
      "source": [
        "This part is a simple map reduce operation where in the mapper, 1 is assigned to each word and in the reducer, all assigned numbers are summed for each key value.\n",
        "\n",
        "10 random words and their number of occurances are shown in the output.\n",
        "\n",
        "Full result is saved in '1_1_results' folder in 4 files. (each worker outputs one file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5SZhWuWiDU-",
        "outputId": "7d1eb509-ab78-43b1-a2b3-f77c1dd0cf33"
      },
      "source": [
        "word_counts = words.map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Occurance of each word\n",
        "print(word_counts.take(10))\n",
        "\n",
        "# Save to file\n",
        "word_counts.saveAsTextFile(\"1_1_results\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('are', 32), ('learning', 3), ('in', 128), ('them', 33), ('work', 3), ('concepts', 4), ('of', 162), ('perfect', 2), ('negotiation', 2), ('persuasion', 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4SmBrPstK6"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK5MhxXEUQd3"
      },
      "source": [
        "Because all words are already lowercased, I've only filtered all the words by the condition of starting with 'm' and shown 10 words in output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJqwd74MiGRx",
        "outputId": "6282b0f6-79e7-4428-bb0e-4cb0b8457c03"
      },
      "source": [
        "# We lowercased all words so only need to check if it is lowercase 'm'def startwith_m(text):\n",
        "m_s = words.filter(lambda x: x.startswith((\"m\"))\n",
        "m_s.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['make', 'make', 'mastery', 'many', 'me', 'make', 'me', 'me', 'my', 'more']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStqsIoa0U7r"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvqCUpy5Uzyg"
      },
      "source": [
        "First, only 5 character words are kept by filtering. There are 576 of them and some of them are shown in the first part of the output.\n",
        "\n",
        "By adding another filter, we can only keep words that do not start with vowels.\n",
        "\n",
        "At the end, the words are sorted and they are shown in the output distinctively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyxyqQmxmX78",
        "outputId": "22466678-bfa3-445d-9e17-58e31d128381"
      },
      "source": [
        "words_5 = words.filter(lambda x: len(x) == 5)\n",
        "print(\"5 character words:\")\n",
        "print(words_5.count())\n",
        "print(words_5.take(5))\n",
        "print(20*\"*\")\n",
        "\n",
        "not_vowel_5 = words_5.filter(lambda x: not x.startswith((\"a\", \"e\", \"i\", \"o\", \"u\")))\n",
        "not_vowel_5.sortBy(lambda x: x).distinct().take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 character words:\n",
            "576\n",
            "['games', 'happy', 'these', 'games', 'class']\n",
            "********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bonus',\n",
              " 'buddy',\n",
              " 'cards',\n",
              " 'faith',\n",
              " 'goods',\n",
              " 'haven',\n",
              " 'likes',\n",
              " 'magog',\n",
              " 'money',\n",
              " 'novel']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAXtjZ50Wgp"
      },
      "source": [
        "## Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gU3akOVlQl"
      },
      "source": [
        "Stop words are found by sorting and keeping the first 10% of Part 1's output. They are nothing but the most occuring words and their counts. \n",
        "\n",
        "10 most repeated words are shown in the output\n",
        "\n",
        "I've discarded the counts at the end to have a list of stop_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvzedxqe9KZ3",
        "outputId": "05a15644-dcb1-4b5a-bd6d-454e715aaf3a"
      },
      "source": [
        "# Descending sort on number of occurances, then take first 10%\n",
        "stop_words = word_counts.sortBy(lambda x: -x[1]).take(all_count//10)\n",
        "\n",
        "print(\"10 most repeated words:\")\n",
        "print(stop_words[:10])\n",
        "print(10*\"*\")\n",
        "\n",
        "# Only keep words (remove counts)\n",
        "stop_words = [i[0] for i in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 most repeated words:\n",
            "[('the', 335), ('and', 238), ('of', 162), ('to', 153), ('in', 128), ('a', 106), ('is', 76), ('this', 74), ('people', 62), ('that', 60)]\n",
            "**********\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll5YWnB7WU_3"
      },
      "source": [
        "In this part, the text file is read again but passed to a different preprocessing function.\n",
        "\n",
        "In this function, all words are lowercased, unalphabetic words are removed and stop_words are removed directly from the lines. All this process is done in line level, not word level!\n",
        "\n",
        "'book' is a list of sentences or lines that don't contain any stopwords.\n",
        "\n",
        "Full result is saved in '1_4_results' folder and partial result is shown in the output below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXG8NQ5XlEUz",
        "outputId": "c579e05d-4e33-44b0-a570-ab1438502790"
      },
      "source": [
        "\n",
        "# Remove unalphabetical characters and stopwords from each line\n",
        "def remove_stopwords_keep_alphabetic(text):\n",
        "  # lowercase, Keep alphabetic, and strip extra spaces\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^a-z\\s\\d]', \"\", text)\n",
        "  text = text.strip()\n",
        "  # Remove stopwords\n",
        "  words = [word for word in text.split(\" \") if word not in stop_words]\n",
        "  text = \" \".join(words)\n",
        "  return text\n",
        "\n",
        "# Read lines and apply preprocessing\n",
        "book = spark.sparkContext.textFile('input.txt', 4).map(remove_stopwords_keep_alphabetic)\n",
        "\n",
        "# Save to file\n",
        "book.saveAsTextFile(\"1_4_results\")\n",
        "\n",
        "book.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 most repeated words:\n",
            "[('the', 335), ('and', 238), ('of', 162), ('to', 153), ('in', 128), ('a', 106), ('is', 76), ('this', 74), ('people', 62), ('that', 60)]\n",
            "**********\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['involved happy loosened learn reinforcing talked familiar lectures spin test our peers able connection framing extent subject practice mastery rufo shown us anything total car salesman goes names purposes referred win lose precursor debate off count several overlapped discuss strongest admitting straight stronger persuading easier competition excel played strong focused',\n",
              " 'instantly facet exceed exactly truth question saying conversation passionate nod affirm slip table winning card everybody wins gonna x minute write y reap rewards seem exaggeration secondly lied expect sort expectation later outlandish believing',\n",
              " 'leads competitors huge leg group slowly suggesting moves explaining sway results favour cards too won practise ganged graded round kicked sure isnt tarnished reason thats gotten reputation accurate burned classmates wolf blend audible members speaking quiet classmate wolves acted try swaying kill victory townspeople pretty appropriately',\n",
              " 'defeat collaborating faster equal regulations extremely team outed identified threat collaborated gone bitter concept extra credit bonus points cant catch basically competitive',\n",
              " 'pay attention calss mastering tool rather merely method conveying practical applications learned bidding convincing fifty',\n",
              " '1 fantastic portrayal pain',\n",
              " '',\n",
              " 'graphic novel name directed complex weaving tales difficulty racist perspectives cultures visually reminiscent comic book gets artistic helps audience grapple troubling topics transcends autobiography magical realism although portrayed playful lightness draw provide approach somewhat material',\n",
              " '',\n",
              " '1970s brief explanation causes effects outlines regular citizens dealt rapidly changing society follows thirteen twentythree follow conflict view inquisitive asks jumps conclusions herself type wants amid grows becomes adult confident new oppressive segregated']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt-lP_QV9WWW"
      },
      "source": [
        "## Part 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoe08k1nXLgE"
      },
      "source": [
        "The text file is read and passed to the preprocessing function of part 0.\n",
        "\n",
        "'book' is a list of sentences or lines.\n",
        "\n",
        "First, book is flatMapped to bigrams by my self-defined function which takes a line, and returns a list of tuples containing neighbor words.\n",
        "\n",
        "Then, word-counts are calculated by a simple map-reduce and the bigrams occuring more than once are kept, sorted and shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4AeJ19r27Jr",
        "outputId": "5ea3bfdc-ca8b-4c89-b70c-5ad560cc09fa"
      },
      "source": [
        "# This function splits sentence and returns bigrams \n",
        "def split(line):\n",
        "    words = line.split(\" \")\n",
        "    return [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "book = spark.sparkContext.textFile('input.txt', 4).map(removePunctuation)\n",
        "\n",
        "bigrams = book.flatMap(lambda line: split(line)) \\\n",
        "              .map(lambda bigram: (bigram, 1)) \\\n",
        "              .reduceByKey(lambda a, b: a+b) \\\n",
        "              .filter(lambda x: x[1] > 1) \\\n",
        "              .sortBy(lambda x: -x[1])\n",
        "\n",
        "bigrams.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('of', 'the'), 56),\n",
              " (('in', 'the'), 53),\n",
              " (('and', 'the'), 15),\n",
              " (('this', 'is'), 15),\n",
              " (('all', 'of'), 13),\n",
              " (('it', 'is'), 13),\n",
              " (('on', 'the'), 12),\n",
              " (('to', 'the'), 11),\n",
              " (('soho', 'square'), 11),\n",
              " (('of', 'this'), 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    }
  ]
}