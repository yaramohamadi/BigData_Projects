{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BD3_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJbNkUoV6uIu"
      },
      "source": [
        "## Getting Spark Ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgYHOzwz-FSg"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFdeevXX-Yd1"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2lJZgiZC-aAw",
        "outputId": "6ee297e5-9cbf-41a1-8cba-e1db93d3fa47"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://64e4511a751f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f53888d73d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3abpwkFuxx"
      },
      "source": [
        "## Part 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t2zbt8z6q5R"
      },
      "source": [
        "#### Read last day and preprocess\n",
        "\n",
        "The column names are persian and cause problems in spark. So they are all translated to english and the types are casted to appropriate types like int, float, long, datetime, ....\n",
        "\n",
        "We can see Schema and the table after the preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny4CN882JLnH",
        "outputId": "78e0de4b-6468-4ab7-9545-b0c120972eeb"
      },
      "source": [
        "from pyspark.sql.functions import col, split, substring, length, to_date, to_timestamp\n",
        "\n",
        "# Read last day\n",
        "df = spark.read.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/BD3/stock/1399-12-27.csv\")\n",
        "\n",
        "df = df.withColumn(\"symbol\",col('نماد')).drop('نماد') \\\n",
        "    .withColumn(\"name\", col('نام')).drop('نام') \\\n",
        "    .withColumn(\"quantity\", col('تعداد').cast('int')).drop('تعداد') \\\n",
        "    .withColumn(\"volume\", col('حجم').cast('int')).drop('حجم') \\\n",
        "    .withColumn(\"value\", col('ارزش').cast('long')).drop('ارزش') \\\n",
        "    .withColumn(\"yesterday\", col('دیروز').cast('int')).drop('دیروز') \\\n",
        "    .withColumn(\"first\", col('اولین').cast('int')).drop('اولین') \\\n",
        "    .withColumn(\"last_trade_value\", col('آخرین معامله - مقدار').cast('int')).drop('آخرین معامله - مقدار') \\\n",
        "    .withColumn(\"last_trade_change\", col('آخرین معامله - تغییر').cast('int')).drop('آخرین معامله - تغییر') \\\n",
        "    .withColumn(\"last_trade_percent\", col('آخرین معامله - درصد').cast('float')).drop('آخرین معامله - درصد') \\\n",
        "    .withColumn(\"last_price_value\", col('قیمت پایانی - مقدار').cast('int')).drop('قیمت پایانی - مقدار') \\\n",
        "    .withColumn(\"last_price_change\", col('قیمت پایانی - تغییر').cast('int')).drop('قیمت پایانی - تغییر') \\\n",
        "    .withColumn(\"last_price_percent\", col('قیمت پایانی - درصد').cast('float')).drop('قیمت پایانی - درصد') \\\n",
        "    .withColumn(\"lowest\", col('کمترین').cast('int')).drop('کمترین') \\\n",
        "    .withColumn(\"highest\", col('بیشترین').cast('int')).drop('بیشترین') \\\n",
        "    .drop(\"_c0\") \\\n",
        "    .withColumn(\"date\", to_date('date', 'dd-MM-yyyy')) \n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- volume: integer (nullable = true)\n",
            " |-- value: integer (nullable = true)\n",
            " |-- yesterday: integer (nullable = true)\n",
            " |-- first: integer (nullable = true)\n",
            " |-- last_trade_value: integer (nullable = true)\n",
            " |-- last_trade_change: integer (nullable = true)\n",
            " |-- last_trade_percent: float (nullable = true)\n",
            " |-- last_price_value: integer (nullable = true)\n",
            " |-- last_price_change: integer (nullable = true)\n",
            " |-- last_price_percent: float (nullable = true)\n",
            " |-- lowest: integer (nullable = true)\n",
            " |-- highest: integer (nullable = true)\n",
            "\n",
            "+----+--------+-----------------------------+--------+---------+-----+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "|date|symbol  |name                         |quantity|volume   |value|yesterday|first |last_trade_value|last_trade_change|last_trade_percent|last_price_value|last_price_change|last_price_percent|lowest|highest|\n",
            "+----+--------+-----------------------------+--------+---------+-----+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "|null|پالايش  |صندوق پالايشي يکم-سهام       |50519   |54374903 |null |86360    |87260 |84650           |-1710            |-1.98             |85740           |-620             |-0.72             |84010 |87650  |\n",
            "|null|رافزا   |رايان هم افزا                |41939   |4993972  |null |90267    |88462 |95683           |5416             |6.0               |92273           |2006             |2.22              |88462 |95683  |\n",
            "|null|شپنا    |پالايش نفت اصفهان            |28447   |174304778|null |14180    |14850 |14590           |410              |2.89              |14670           |490              |3.46              |14330 |14990  |\n",
            "|null|اپال    |فرآوري معدني اپال كاني پارس  |28125   |47994924 |null |18080    |17980 |17970           |-110             |-0.61             |17900           |-180             |-1.0              |17720 |18100  |\n",
            "|null|دارا يكم|صندوق واسطه گري مالي يكم-سهام|26796   |19008310 |null |147990   |150120|146610          |-1380            |-0.93             |148210          |220              |0.15              |146040|151590 |\n",
            "+----+--------+-----------------------------+--------+---------+-----+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NerQ86MMHT-n"
      },
      "source": [
        "#### Dataframe\n",
        "\n",
        "To get the cheapest or priciest stocks, we order the table by 'last_price_value' and show the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3IR1cbQ-bj9",
        "outputId": "47c3b750-b9e6-40b2-a43d-b78754bef06f"
      },
      "source": [
        "# Highest price\n",
        "df.orderBy('last_price_value', ascending = False).select('name', 'last_price_value').show(5, truncate=False)\n",
        "\n",
        "# Lowest price\n",
        "df.orderBy('last_price_value', ascending = True).select('name', 'last_price_value').show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------+----------------+\n",
            "|name                         |last_price_value|\n",
            "+-----------------------------+----------------+\n",
            "|مشاركت دولتي1-شرايط خاص001026|990000          |\n",
            "|سپنتا                        |410750          |\n",
            "|خوراك‌  دام‌ پارس‌           |400550          |\n",
            "|صنايع‌جوشكاب‌يزد             |360970          |\n",
            "|پتروشيمي فناوران             |327430          |\n",
            "+-----------------------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------+----------------+\n",
            "|name                          |last_price_value|\n",
            "+------------------------------+----------------+\n",
            "|صندوق س. با درآمد ثابت كيان   |1               |\n",
            "|صندوق س.اعتماد آفرين پارسيان-د|1               |\n",
            "|صندوق س ياقوت آگاه-ثابت       |1               |\n",
            "|صندوق س. با درآمد ثابت كمند   |1               |\n",
            "|صندوق س نگين سامان-ثابت       |1               |\n",
            "+------------------------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0jf5IjVHXTu"
      },
      "source": [
        "#### SQL\n",
        "\n",
        "To execute SQL code we should create a temporary table from our dataframe.\n",
        "\n",
        "The SQL code just orders by 'last_price_value' and shows first 5 results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyTd-LHtHhFf",
        "outputId": "c2c66b69-26a4-4dea-aefd-d706ba6601ac"
      },
      "source": [
        "# Create temporary table\n",
        "df.registerTempTable('sqltable')\n",
        "\n",
        "# max\n",
        "newDF = spark.sql(\"\"\"\n",
        "  select name, last_price_value \n",
        "  from sqltable \n",
        "  order by last_price_value desc \n",
        "  limit 5\n",
        "\"\"\")\n",
        "newDF.show(truncate = False)\n",
        "\n",
        "# min\n",
        "newDF = spark.sql(\"\"\"\n",
        "  select name, last_price_value \n",
        "  from sqltable \n",
        "  order by last_price_value asc \n",
        "  limit 5\n",
        "\"\"\")\n",
        "newDF.show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------+----------------+\n",
            "|name                         |last_price_value|\n",
            "+-----------------------------+----------------+\n",
            "|مشاركت دولتي1-شرايط خاص001026|990000          |\n",
            "|سپنتا                        |410750          |\n",
            "|خوراك‌  دام‌ پارس‌           |400550          |\n",
            "|صنايع‌جوشكاب‌يزد             |360970          |\n",
            "|پتروشيمي فناوران             |327430          |\n",
            "+-----------------------------+----------------+\n",
            "\n",
            "+---------------------------+----------------+\n",
            "|name                       |last_price_value|\n",
            "+---------------------------+----------------+\n",
            "|صندوق س. با درآمد ثابت كمند|1               |\n",
            "|صندوق س. با درآمد ثابت كيان|1               |\n",
            "|صندوق س. آرمان آتي كوثر-د  |1               |\n",
            "|صندوق س ياقوت آگاه-ثابت    |1               |\n",
            "|صندوق س نگين سامان-ثابت    |1               |\n",
            "+---------------------------+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn9y3DUNQl5t"
      },
      "source": [
        "## Part 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcjRnCP6nLy"
      },
      "source": [
        "\n",
        "#### Read all CSVs\n",
        "\n",
        "The results are for 2 months instead of 6 months as the question asks.\n",
        "\n",
        "The same preprocessing is done, only this time on all of the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbs0CiqSFLR5",
        "outputId": "95499b07-ac4a-4444-ae85-d1a9a9806285"
      },
      "source": [
        "from pyspark.sql.functions import col, split, substring, length, to_date, to_timestamp\n",
        "\n",
        "# Read all files\n",
        "df = spark.read.option(\"header\", \"true\").csv(\"/content/drive/MyDrive/BD3/stock/*.csv\")\n",
        "\n",
        "# Preprocess\n",
        "df = df.withColumn(\"symbol\",col('نماد')).drop('نماد') \\\n",
        "    .withColumn(\"name\", col('نام')).drop('نام') \\\n",
        "    .withColumn(\"quantity\", col('تعداد').cast('int')).drop('تعداد') \\\n",
        "    .withColumn(\"volume\", col('حجم').cast('int')).drop('حجم') \\\n",
        "    .withColumn(\"value\", col('ارزش').cast('long')).drop('ارزش') \\\n",
        "    .withColumn(\"yesterday\", col('دیروز').cast('int')).drop('دیروز') \\\n",
        "    .withColumn(\"first\", col('اولین').cast('int')).drop('اولین') \\\n",
        "    .withColumn(\"last_trade_value\", col('آخرین معامله - مقدار').cast('int')).drop('آخرین معامله - مقدار') \\\n",
        "    .withColumn(\"last_trade_change\", col('آخرین معامله - تغییر').cast('int')).drop('آخرین معامله - تغییر') \\\n",
        "    .withColumn(\"last_trade_percent\", col('آخرین معامله - درصد').cast('float')).drop('آخرین معامله - درصد') \\\n",
        "    .withColumn(\"last_price_value\", col('قیمت پایانی - مقدار').cast('int')).drop('قیمت پایانی - مقدار') \\\n",
        "    .withColumn(\"last_price_change\", col('قیمت پایانی - تغییر').cast('int')).drop('قیمت پایانی - تغییر') \\\n",
        "    .withColumn(\"last_price_percent\", col('قیمت پایانی - درصد').cast('float')).drop('قیمت پایانی - درصد') \\\n",
        "    .withColumn(\"lowest\", col('کمترین').cast('int')).drop('کمترین') \\\n",
        "    .withColumn(\"highest\", col('بیشترین').cast('int')).drop('بیشترین') \\\n",
        "    .drop(\"_c0\") \\\n",
        "    .withColumn(\"date\", to_date('date', 'yyyy-MM-dd')) \n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- volume: integer (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            " |-- yesterday: integer (nullable = true)\n",
            " |-- first: integer (nullable = true)\n",
            " |-- last_trade_value: integer (nullable = true)\n",
            " |-- last_trade_change: integer (nullable = true)\n",
            " |-- last_trade_percent: float (nullable = true)\n",
            " |-- last_price_value: integer (nullable = true)\n",
            " |-- last_price_change: integer (nullable = true)\n",
            " |-- last_price_percent: float (nullable = true)\n",
            " |-- lowest: integer (nullable = true)\n",
            " |-- highest: integer (nullable = true)\n",
            "\n",
            "+----------+--------+-----------------------------+--------+--------+-------------+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "|date      |symbol  |name                         |quantity|volume  |value        |yesterday|first |last_trade_value|last_trade_change|last_trade_percent|last_price_value|last_price_change|last_price_percent|lowest|highest|\n",
            "+----------+--------+-----------------------------+--------+--------+-------------+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "|1399-12-26|پالايش  |صندوق پالايشي يکم-سهام       |57364   |66618974|5753457915230|84380    |84500 |86470           |2090             |2.48              |86360           |1980             |2.35              |83500 |87750  |\n",
            "|1399-12-26|بركت    |گروه دارويي بركت             |44775   |98415477|3401132671930|34900    |35500 |34210           |-690             |-1.98             |34560           |-340             |-0.97             |34210 |36170  |\n",
            "|1399-12-26|دارا يكم|صندوق واسطه گري مالي يكم-سهام|40813   |33622663|4975722543670|145790   |143500|149000          |3210             |2.2               |147990          |2200             |1.51              |142210|150280 |\n",
            "|1399-12-26|رافزا   |رايان هم افزا                |30057   |17651115|1593319677261|86269    |88500 |91445           |5176             |6.0               |90267           |3998             |4.63              |87210 |91445  |\n",
            "|1399-12-26|كزغال   |زغال سنگ پروده طبس           |28959   |12768516|473717842605 |37078    |36860 |37110           |32               |0.09              |37100           |22               |0.06              |36440 |38000  |\n",
            "+----------+--------+-----------------------------+--------+--------+-------------+---------+------+----------------+-----------------+------------------+----------------+-----------------+------------------+------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yps9rsM1RAu9"
      },
      "source": [
        "#### DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_CRiqvWl_vY"
      },
      "source": [
        "Volumes are summed for each symbol and the biggest is shown\n",
        "\n",
        "it is خساپا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUD1O7baGHJx",
        "outputId": "9a2dc066-e01a-4b70-fe22-92057021f972"
      },
      "source": [
        "df.groupBy('symbol').sum('volume') \\\n",
        "       .orderBy('sum(volume)', ascending=False) \\\n",
        "       .first()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(symbol='خساپا', sum(volume)=28093676706)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqxSc0wRtRl"
      },
      "source": [
        "#### SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQHpdxHmOLp"
      },
      "source": [
        "Volumes are summed, grouped by symbol and sorted by the sum.\n",
        "\n",
        "Highest volume belongs to خساپا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5_ZZqQXJisf",
        "outputId": "b19b0234-8b72-4282-9a49-826ec1a143ee"
      },
      "source": [
        "# Create temporary table\n",
        "df.registerTempTable('sqltable')\n",
        "\n",
        "# max\n",
        "newDF = spark.sql(\"\"\"\n",
        "  select symbol, sum(volume) as total_volume\n",
        "  from sqltable \n",
        "  group by symbol\n",
        "  order by total_volume desc\n",
        "  limit 1\n",
        "\"\"\")\n",
        "newDF.show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------+\n",
            "|symbol|total_volume|\n",
            "+------+------------+\n",
            "|خساپا |28093676706 |\n",
            "+------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adiunENGmNbA"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWHQgDDcmOh4"
      },
      "source": [
        "#### DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsOOJimAmgLg"
      },
      "source": [
        "In this question, we only need year, month, date, symbol and value. So at first, I only select these columns.\n",
        "\n",
        "First, the dataframe is self-joined with all the records of each symbol that are ahead but in the same month. These conditions are met in filter() after the join. Then, the increase in value is calculated for each record and only maximums in each [month, symbol] group are kept. \n",
        "\n",
        "Then, because we want to show top 10 records for each month group, we need toa kind of partitioning by month and ordering by maximum increase value. So, a new row number column is created that represents the rank of each symbol in that month with respect to maximum increase.\n",
        "\n",
        "At last, we only output records with row numbers not more than 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nflq4HbMSN6n",
        "outputId": "0ef804e1-5bd5-46d3-cfc5-b3495d47ce13"
      },
      "source": [
        "from pyspark.sql.functions import year, month, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Keep only columns that we need\n",
        "df_tmp = df.select(year('date').alias('year'), \n",
        "          month('date').alias('month'), \n",
        "          'date', \n",
        "          'symbol',\n",
        "          'value')\n",
        "\n",
        "# should only show first 10 records for each month!  use Window function:\n",
        "windowSpec  = Window.partitionBy(\"year\", 'month').orderBy(col(\"max(increase)\").desc())\n",
        "\n",
        "# Self join with all records of each symbol that are ahead but in the same month\n",
        "# new column: increase amount  \n",
        "# get maximum increase for each symbol for each month\n",
        "# ascending order by time, descending order by maximum increase value\n",
        "df_tmp = df_tmp.alias('df_1').join(df_tmp.alias('df_2')) \\\n",
        "                              .filter((col('df_1.date') < col('df_2.date'))\n",
        "                                  & (col(\"df_1.symbol\") == col(\"df_2.symbol\"))\n",
        "                                  & (col(\"df_1.month\") == col(\"df_2.month\"))\n",
        "                                  & (col(\"df_1.year\") == col(\"df_2.year\"))) \\\n",
        "                              .withColumn(\"increase\", col('df_2.value') - col('df_1.value')) \\\n",
        "                              .groupBy(col('df_1.year'), col('df_1.month'), col('df_1.symbol')) \\\n",
        "                              .max(\"increase\") \\\n",
        "                              .sort(col('year'), col('month')) \\\n",
        "                              .withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "                              .filter(col('row_number') < 11) \\\n",
        "                              .show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+--------+--------------+----------+\n",
            "|year|month|symbol  |max(increase) |row_number|\n",
            "+----+-----+--------+--------------+----------+\n",
            "|1399|11   |پالايش  |16759618165030|1         |\n",
            "|1399|11   |دارا يكم|12175656560440|2         |\n",
            "|1399|11   |شستا    |9800328189440 |3         |\n",
            "|1399|11   |شپنا    |8729919981050 |4         |\n",
            "|1399|11   |اوصتا4  |7749230000000 |5         |\n",
            "|1399|11   |خساپا   |7685442557110 |6         |\n",
            "|1399|11   |امين يكم|7125258897273 |7         |\n",
            "|1399|11   |فولاد2  |6700200000000 |8         |\n",
            "|1399|11   |فملي    |5283092056990 |9         |\n",
            "|1399|11   |وتجارت  |4361376265020 |10        |\n",
            "|1399|12   |شپنا4   |16817486114140|1         |\n",
            "|1399|12   |پالايش  |11624833080670|2         |\n",
            "|1399|12   |وغدير2  |7955327200000 |3         |\n",
            "|1399|12   |اپال    |7889415735420 |4         |\n",
            "|1399|12   |دارا يكم|7450902080070 |5         |\n",
            "|1399|12   |بركت    |6551245134860 |6         |\n",
            "|1399|12   |امين يكم|6135563904277 |7         |\n",
            "|1399|12   |كمند    |5070727437865 |8         |\n",
            "|1399|12   |شپنا    |4854738538410 |9         |\n",
            "|1399|12   |اوصتا4  |4694149900000 |10        |\n",
            "+----+-----+--------+--------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacW_4ROduX6"
      },
      "source": [
        "#### SQL\n",
        "\n",
        "Similar to the previous code, each record is joined with records of same symbol that are ahead but in the same month and maximum increase of value for each month and symbol is found.\n",
        "\n",
        "In order to only show top 10 increases in each month, a row number is created and only records with row number not bigger than 10 are shown in output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z1M_p1EWa22",
        "outputId": "82ba26d2-6124-4f54-f034-e9a57a710c14"
      },
      "source": [
        "# Create temporary table\n",
        "df.registerTempTable('sqltable')\n",
        "\n",
        "newDF = spark.sql(\"\"\"\n",
        "with stock as (\n",
        "select  year(s1.date) as year, month(s1.date) as month, s1.symbol as symbol, max(s2.value - s1.value) as maximum_increase\n",
        "from sqltable as s1 join sqltable as s2\n",
        "where s1.symbol == s2.symbol and s1.date < s2.date and month(s1.date) == month(s1.date) and year(s1.date) == year(s1.date)\n",
        "group by year(s1.date), month(s1.date), s1.symbol\n",
        "order by year asc, month asc\n",
        "),\n",
        "stock2 as \n",
        "(select *, ROW_NUMBER() over(partition by year, month order by maximum_increase desc) as rnn\n",
        "from stock\n",
        ")\n",
        "select * \n",
        "from stock2\n",
        "where rnn <= 10\n",
        "\"\"\")\n",
        "\n",
        "newDF.show(truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+--------+----------------+---+\n",
            "|year|month|symbol  |maximum_increase|rnn|\n",
            "+----+-----+--------+----------------+---+\n",
            "|1399|11   |پالايش  |16759618165030  |1  |\n",
            "|1399|11   |دارا يكم|12175656560440  |2  |\n",
            "|1399|11   |امين يكم|9861407653066   |3  |\n",
            "|1399|11   |شستا    |9800328189440   |4  |\n",
            "|1399|11   |شپنا    |8729919981050   |5  |\n",
            "|1399|11   |اوصتا4  |7749230000000   |6  |\n",
            "|1399|11   |خساپا   |7685442557110   |7  |\n",
            "|1399|11   |فولاد2  |6700200000000   |8  |\n",
            "|1399|11   |بركت    |6452484770120   |9  |\n",
            "|1399|11   |فملي    |5283092056990   |10 |\n",
            "|1399|12   |شپنا4   |16817486114140  |1  |\n",
            "|1399|12   |پالايش  |11624833080670  |2  |\n",
            "|1399|12   |وغدير2  |7955327200000   |3  |\n",
            "|1399|12   |اپال    |7889415735420   |4  |\n",
            "|1399|12   |دارا يكم|7450902080070   |5  |\n",
            "|1399|12   |بركت    |6551245134860   |6  |\n",
            "|1399|12   |امين يكم|6135563904277   |7  |\n",
            "|1399|12   |كمند    |5070727437865   |8  |\n",
            "|1399|12   |شپنا    |4854738538410   |9  |\n",
            "|1399|12   |اوصتا4  |4694149900000   |10 |\n",
            "+----+-----+--------+----------------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgkO2Oxkxgm0"
      },
      "source": [
        "## Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDHSXzXYxiSM"
      },
      "source": [
        "#### DataFrame\n",
        "\n",
        "Similar to the last question, we self-join the dataframe with records of the same symbol that are ahead (any month) and keep the price decreases in a new column. There is a new constraint on the join. By using 'yesterday' column, we make sure the price has not increased from yesterday to today! (ریزش) \n",
        "\n",
        "Then we find the maximum decrease of each symbol and show the results in descending order\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KZO7yctXSPT",
        "outputId": "e97dbf74-fd23-4450-e174-802afb3ad508"
      },
      "source": [
        "from pyspark.sql.functions import year, month, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# we should check price of the day before and make sure it has not increased!\n",
        "# Keep only columns that we need\n",
        "df_tmp = df.select('date', 'symbol', 'last_price_value', 'yesterday')\n",
        "\n",
        "# Self join with all records of each symbol that are ahead (to calculate max decrease)\n",
        "# add condition: value has not increased from yesterday\n",
        "df_tmp = df_tmp.alias('df_1').join(df_tmp.alias('df_2')) \\\n",
        "                              .filter((col('df_1.date') < col('df_2.date'))\n",
        "                                  & (col(\"df_1.symbol\") == col(\"df_2.symbol\"))\n",
        "                                  & (col(\"df_1.yesterday\") >= col(\"df_1.last_price_value\"))) \\\n",
        "                              .withColumn(\"decrease\", col('df_1.last_price_value') - col('df_2.last_price_value')) \\\n",
        "                              .groupBy(col('df_1.symbol')) \\\n",
        "                              .max(\"decrease\") \\\n",
        "                              .orderBy(col(\"max(decrease)\").desc()) \\\n",
        "                              .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|symbol  |max(decrease)|\n",
            "+--------+-------------+\n",
            "|سصفها   |276820       |\n",
            "|انرژي3  |120460       |\n",
            "|بكاب    |93710        |\n",
            "|فنورد   |75840        |\n",
            "|غدام    |69180        |\n",
            "|فپنتا   |66670        |\n",
            "|فمراد   |53530        |\n",
            "|شسينا   |51790        |\n",
            "|غچين    |50630        |\n",
            "|وملي    |47490        |\n",
            "|سكارون  |45109        |\n",
            "|كدما    |45020        |\n",
            "|تكنار   |42194        |\n",
            "|دشيري   |42084        |\n",
            "|گكيش    |41150        |\n",
            "|خفولا   |40193        |\n",
            "|واحيا   |37647        |\n",
            "|زبينا   |37304        |\n",
            "|دارا يكم|36810        |\n",
            "|وهور    |36754        |\n",
            "+--------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f1gbwVrpkSA"
      },
      "source": [
        "#### SQL\n",
        "\n",
        "Very similar to last question. However, a new constraint in WHERE clause is added to make sure price of yesterday is bigger than today's price (ریزش) and maximum decrease in found for each symbol and sorted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMi4FKSJXaMg",
        "outputId": "2dccfe78-d77f-48ad-f87b-278d13a0bc43"
      },
      "source": [
        "# Create temporary table\n",
        "df.registerTempTable('sqltable')\n",
        "\n",
        "newDF = spark.sql(\"\"\"\n",
        "select s1.symbol, max(s1.last_price_value - s2.last_price_value) as max_decrease\n",
        "from sqltable as s1 join sqltable as s2\n",
        "where s1.symbol == s2.symbol and s1.date < s2.date and s1.yesterday >= s1.last_price_value\n",
        "group by s1.symbol\n",
        "order by max_decrease desc\n",
        "limit 20\n",
        "\"\"\")\n",
        "\n",
        "newDF.show(truncate = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------------+\n",
            "|symbol  |max_decrease|\n",
            "+--------+------------+\n",
            "|سصفها   |276820      |\n",
            "|انرژي3  |120460      |\n",
            "|بكاب    |93710       |\n",
            "|فنورد   |75840       |\n",
            "|غدام    |69180       |\n",
            "|فپنتا   |66670       |\n",
            "|فمراد   |53530       |\n",
            "|شسينا   |51790       |\n",
            "|غچين    |50630       |\n",
            "|وملي    |47490       |\n",
            "|سكارون  |45109       |\n",
            "|كدما    |45020       |\n",
            "|تكنار   |42194       |\n",
            "|دشيري   |42084       |\n",
            "|گكيش    |41150       |\n",
            "|خفولا   |40193       |\n",
            "|واحيا   |37647       |\n",
            "|زبينا   |37304       |\n",
            "|دارا يكم|36810       |\n",
            "|وهور    |36754       |\n",
            "+--------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4ds8rT76dQT"
      },
      "source": [
        "## Part 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MarDaaDa6e1r"
      },
      "source": [
        "#### DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Se2UITXp8Zq"
      },
      "source": [
        "We should count the records of each symbol where volume is not zero. This gives us number of days that each symbol has open.\n",
        "\n",
        "We sort the counts in ascending order and find the stocks with least open days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfgAylmi6LRM",
        "outputId": "76a1805b-32e7-4415-abd9-c04360a86342"
      },
      "source": [
        "from pyspark.sql.functions import year, month, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df.select('symbol', 'volume') \\\n",
        "  .filter(col('volume') != 0) \\\n",
        "  .groupBy('symbol') \\\n",
        "  .count() \\\n",
        "  .withColumn('days_open', col('count')).drop('count') \\\n",
        "  .orderBy(col('days_open').asc(), col('symbol').asc()) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+\n",
            "| symbol|days_open|\n",
            "+-------+---------+\n",
            "|  آريا2|        1|\n",
            "|  آسام2|        1|\n",
            "|  آسام4|        1|\n",
            "| آينده4|        1|\n",
            "| اتكام2|        1|\n",
            "|  ارزش2|        1|\n",
            "| استارز|        1|\n",
            "|  افاد4|        1|\n",
            "|   افق2|        1|\n",
            "|  اكالا|        1|\n",
            "|  اميد2|        1|\n",
            "|  امين4|        1|\n",
            "| انرژي1|        1|\n",
            "| انرژي2|        1|\n",
            "|  اوان2|        1|\n",
            "|  اپال2|        1|\n",
            "|  اپال4|        1|\n",
            "|اپرداز2|        1|\n",
            "|بترانس2|        1|\n",
            "|بزاگرس4|        1|\n",
            "+-------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goBRHHZd6gfJ"
      },
      "source": [
        "#### SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kqu3Y3TqauP"
      },
      "source": [
        "Nothing new here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjQ1wB566hWp",
        "outputId": "ffe655c4-67e2-4af7-abac-c91c90ff31eb"
      },
      "source": [
        "# Create temporary table\n",
        "df.registerTempTable('sqltable')\n",
        "\n",
        "newDF = spark.sql(\"\"\"\n",
        "select symbol, count(volume) as days_open\n",
        "from sqltable \n",
        "where volume <> 0\n",
        "group by symbol \n",
        "order by days_open asc, symbol asc\n",
        "LIMIT 20;\n",
        "\"\"\")\n",
        "\n",
        "newDF.show(truncate = False)S"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+\n",
            "|symbol |days_open|\n",
            "+-------+---------+\n",
            "|آريا2  |1        |\n",
            "|آسام2  |1        |\n",
            "|آسام4  |1        |\n",
            "|آينده4 |1        |\n",
            "|اتكام2 |1        |\n",
            "|ارزش2  |1        |\n",
            "|استارز |1        |\n",
            "|افاد4  |1        |\n",
            "|افق2   |1        |\n",
            "|اكالا  |1        |\n",
            "|اميد2  |1        |\n",
            "|امين4  |1        |\n",
            "|انرژي1 |1        |\n",
            "|انرژي2 |1        |\n",
            "|اوان2  |1        |\n",
            "|اپال2  |1        |\n",
            "|اپال4  |1        |\n",
            "|اپرداز2|1        |\n",
            "|بترانس2|1        |\n",
            "|بزاگرس4|1        |\n",
            "+-------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}